# Download with whl
## 搜索网址
[https://github.com/Dao-AILab/flash-attention/releases?page=1](https://github.com/Dao-AILab/flash-attention/releases?page=1)
## 寻找版本
找到对应得的python版本，CUDA版本和torch版本，例如：flash_attn-2.5.7+cu122torch2.1cxx11abiFALSE-cp310-cp310-linux_x86_64.whl
